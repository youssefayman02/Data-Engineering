{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1 - EDA and Preprocessing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make sure to include markdown-based text commenting and explaining each step you perform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Libraries for EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "# Data manipulation and preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "\n",
    "# Cool plotting style\n",
    "plt.style.use('ggplot')\n",
    "rcParams['figure.figsize'] = 12, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set an option to display all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(data_path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/fintech_data_22_52_14669.csv'\n",
    "df = load_data(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the first 5 rows of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the last 5 rows of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the size of the dataset (rows, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting short summary of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting statistical summary of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing correlation between the columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to plot the data for visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(df: pd.DataFrame, figsize: tuple):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(df.corr(), \n",
    "                cbar=True, \n",
    "                annot=True, \n",
    "                square=True, \n",
    "                cmap='Spectral_r', \n",
    "                fmt='.2f', \n",
    "                linewidths=2,\n",
    "                annot_kws={'size': 15})\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram(df: pd.DataFrame, column_name: str, kde, figsize: tuple):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.histplot(df[column_name], kde=kde)\n",
    "    plt.title(f'Distribution of {column_name}')\n",
    "    plt.show()\n",
    "\n",
    "def plot_boxplot_single_column(df: pd.DataFrame, column_name: str, figsize: tuple):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.boxplot(x=column_name, data=df)\n",
    "    plt.title(f'Boxplot of {column_name}')\n",
    "    plt.show()\n",
    "\n",
    "def plot_boxplot_multiple_columns(df: pd.DataFrame, \n",
    "                                  column_name1:str, \n",
    "                                  column_name2: str, \n",
    "                                  title: str, \n",
    "                                  x_label: str, \n",
    "                                  y_label: str, \n",
    "                                  figsize: tuple):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.boxplot(x=column_name1, y=column_name2, data=df)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.show()\n",
    "\n",
    "def plot_vertical_countplot(df: pd.DataFrame, column_name: str, figsize: tuple):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.countplot(x=column_name, data=df)\n",
    "    plt.title(f'Countplot of {column_name}')\n",
    "    plt.show()\n",
    "\n",
    "def plot_horizontal_countplot(df: pd.DataFrame, column_name: str, figsize: tuple):\n",
    "    plt.figure(figsize=figsize)\n",
    "    # sorted\n",
    "    sns.countplot(y=column_name, data=df, order = df[column_name].value_counts().index)\n",
    "    plt.title(f'Countplot of {column_name}')\n",
    "    plt.show()\n",
    "\n",
    "def plot_countplot_multiple_columns(df: pd.DataFrame, \n",
    "                                  column_name1:str, \n",
    "                                  column_name2: str, \n",
    "                                  title: str, \n",
    "                                  x_label: str, \n",
    "                                  y_label: str, \n",
    "                                  figsize: tuple):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.countplot(data=df, x=column_name1, hue=column_name2, palette='Set2')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(column_name1)\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title=column_name2)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_top_n_barplot(df: pd.DataFrame, n: int, title: str, figsize: tuple):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.barplot(x=df.values[:n], y=df.index[:n])\n",
    "    plt.title(f'Top {n} {title}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Category')\n",
    "    plt.show()\n",
    "\n",
    "def plot_scatterplot(df: pd.DataFrame, x:str, y:str, figsize: tuple):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.scatterplot(x=x, y=y, data=df)\n",
    "    plt.title(f'{x} vs {y}')\n",
    "    plt.show()\n",
    "\n",
    "def plot_scatter_average(df: pd.DataFrame, columnName1: str, columnName2: str, figsize: tuple):\n",
    "  average = df.groupby(columnName1)[columnName2].mean()\n",
    "  plt.figure(figsize=figsize)\n",
    "  plt.xlabel(columnName1)\n",
    "  plt.ylabel(columnName2)\n",
    "  plt.title(columnName2+' VS. '+columnName1)\n",
    "  plt.scatter(average.index,average)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing Relationship between features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying correlation between features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_matrix(df, (12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q1: What is the distribution loan amount among customers?\n",
    "\n",
    "Answer: The distribution of loan amount among customers is right-skewed, with most customers having a loan amount of around 10000. We can observe that most loans fall within a specific range indicating that customers tend to borrow moderate amounts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(df, column_name='Loan Amount', kde=True, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot_single_column(df, column_name='Loan Amount', figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q2: What is the top 10 employee title that issue a loan?\n",
    "\n",
    "Answer: We can observe that the top 10 employee titles that issue loans are Teacher, Manager, Owner, Registered Nurse, Driver, Supervisor, Sales, Project Manager and Office Manager.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_n_barplot(df['Emp Title'].value_counts(), 10, 'Employment Title', figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q3: What is the distribution of the number of years of employment among customers?\n",
    "\n",
    "Answer: We can observe that employees with 10 years of employment are the most common among customers. The most customers that issue loan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vertical_countplot(df, 'Emp Length', figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q4: What is the distribution of the loan amount among customers with different employment lengths?\n",
    "\n",
    "Answer: Loan amounts do not vary significantly by employment length, but there is a slight increase in loan amounts for customers with longer employment. Customers with very short employment lengths also have smaller loan amounts on average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot_multiple_columns(df, 'Emp Length', 'Loan Amount', 'Loan Amount Distribution by Employment Length', 'Employment Length', 'Loan Amount', (12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q5: What is the relationship between the annual income and loan amount?\n",
    "\n",
    "Answer: We can observe that there seems to be a positive trend, higher-income individuals generally take larger loans, yet the correlation is not strictly linear, suggesting that other factors may also influence loan amount.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship between annual income and loan amount\n",
    "plot_scatterplot(df, 'Annual Inc', 'Loan Amount', figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q6: Are most of the customer's income and employment verified?\n",
    "\n",
    "Answer: We can observe that there is a high percentage of customers whose income and employment are not verified. This could be a potential risk that these customers may not be able to repay the loan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vertical_countplot(df, 'Verification Status', figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q7: What is the geographical distribution of customers based on state?\n",
    "\n",
    "Answer: We can observe that the top 5 states with the highest number of borrowers are California, Texas, New York, Florida, and Illinois.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_countplot(df, 'Addr State', figsize=(12, 12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q8: What is the distribution of the loan status\n",
    "\n",
    "Answer: We can observe that most loans are current, followed by fully paid. This indicates that most customers are able to repay their loans(i.e most of the loans are either actively being repaid or have been fully repaid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vertical_countplot(df, 'Loan Status', figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q9: What is the distribution of the loan status based on the loan grade?\n",
    "\n",
    "Answer: We can observe that most loans are grade B, followed by grade C. Most of the loans are current, followed by fully paid. This indicates that most customers are able to repay their loans(i.e most of the loans are either actively being repaid or have been fully repaid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_countplot_multiple_columns(df, 'Grade', 'Loan Status', 'Loan Status by Grade', 'Grade', 'Count', (12, 6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q10: Is the loan amount affected by the loan grade?\n",
    "\n",
    "Answer: We can observe that classes E,F,G have higher loan amounts compared to other classes. This indicates that customers with lower loan grades tend to borrow more money.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_average(df, 'Grade','Loan Amount', (12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q11: Is the loan amount the same as the funded amount?\n",
    "\n",
    "Answer: We can observe that the loan amount is almost the same as the funded amount. We can infer that the funded amount is the amount that the customer actually receives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatterplot(df, 'Loan Amount', 'Funded Amount', figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q12: Is there a relationship between the loan grade and the interest rate?\n",
    "\n",
    "Answer: We can observe that higher grade such as (A, B, C) have lower interest rates compared to lower grades such as (D, E, F, G). This indicates that customers with higher grades are less risky and are charged lower interest rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot_multiple_columns(df, 'Grade', 'Int Rate', 'Interest Rate Distribution by Grade', 'Grade', 'Interest Rate', (12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatterplot(df, 'Grade', 'Int Rate', (12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q13: What is the most common purpose for taking a loan?\n",
    "\n",
    "Answer: We can observe that the most common purpose for taking a loan is debt consolidation, followed by credit card. This indicates that most customers take loans to consolidate their debts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_countplot(df, 'Purpose', (12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "- The distribution of loan amount among customers is right-skewed, with most customers having a loan amount of around 10000. We can observe that most loans fall within a specific range indicating that customers tend to borrow moderate amounts.\n",
    "\n",
    "- The top 10 employee titles that issue loans are Teacher, Manager, Owner, Registered Nurse, Driver, Supervisor, Sales, Project Manager, and Office Manager.\n",
    "\n",
    "- Employees with 10 years of employment are the most common among customers.\n",
    "\n",
    "- Loan amounts do not vary significantly by employment length, but there is a slight increase in loan amounts for customers with longer employment. Customers with very short employment lengths also have smaller loan amounts on average.\n",
    "\n",
    "- There seems to be a positive trend between annual income and loan amount, higher-income individuals generally take larger loans, yet the correlation is not strictly linear, suggesting that other factors may also influence loan amount.\n",
    "\n",
    "- There is a high percentage of customers whose income and employment are not verified. This could be a potential risk that these customers may not be able to repay the loan.\n",
    "\n",
    "- The top 5 states with the highest number of borrowers are California, Texas, New York, Florida, and Illinois.\n",
    "\n",
    "- Most loans are current, followed by fully paid. This indicates that most customers are able to repay their loans(i.e most of the loans are either actively being repaid or have been fully repaid)\n",
    "\n",
    "- Most loans are grade B, followed by grade C. Most of the loans are current, followed by fully paid.\n",
    "\n",
    "- Classes E,F,G have higher loan amounts compared to other classes. This indicates that customers with lower loan grades tend to borrow more money.\n",
    "\n",
    "- The loan amount is almost the same as the funded amount. We can infer that the funded amount is the amount that the customer actually receives.\n",
    "\n",
    "- Higher grade such as (A, B, C) have lower interest rates compared to lower grades such as (D, E, F, G). This indicates that customers with higher grades are less risky and are charged lower interest rates.\n",
    "\n",
    "- The most common purpose for taking a loan is debt consolidation, followed by credit card. This indicates that most customers take loans to consolidate their debts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Cleaning Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying up column names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have renamed the columns to make them more readable and consistent, by converting them to lowercase and replacing spaces with underscores and removing special characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_name(column_name: str):\n",
    "    formatted_name = column_name.lower()\n",
    "    formatted_name = formatted_name.strip()\n",
    "    formatted_name = formatted_name.replace(' ', '_')\n",
    "    formatted_name = ''.join(e for e in formatted_name if e.isalnum() or e == '_')\n",
    "\n",
    "    return formatted_name\n",
    "\n",
    "def clean_column_names(df: pd.DataFrame):\n",
    "    df.columns = [clean_column_name(column) for column in df.columns]\n",
    "    return df\n",
    "\n",
    "df = clean_column_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a suitable column index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created a function that returns the candidate columns for the index based on the uniqueness of the values of the columns. I have selected the column 'loan_id' as the index since it is unique and can be used to identify each row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_feature_candidates(df: pd.DataFrame):\n",
    "    return df.shape[0] - df.nunique()\n",
    "\n",
    "index_feature_candidates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have two features that can be used as index so I will use the loan_id as it is more readable than customer id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('loan_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe inconsistent data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there is any duplicates rows in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_column_values(df: pd.DataFrame, columns: list):\n",
    "    \"\"\"\n",
    "    Summarizes the values of specified columns in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The DataFrame to summarize.\n",
    "    columns (list): List of column names to summarize.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A summary DataFrame containing unique values and their counts for each specified column.\n",
    "    \"\"\"\n",
    "    summary_list = []\n",
    "\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            value_counts = df[column].value_counts(dropna=False)\n",
    "            # Create a summary entry for this column\n",
    "            summary_entry = {\n",
    "                'Column': column,\n",
    "                'Total Values': df[column].size,\n",
    "                'Unique Values': value_counts.size,\n",
    "                'Value Counts': value_counts.to_dict()\n",
    "            }\n",
    "            summary_list.append(summary_entry)\n",
    "\n",
    "    return summary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate_rows(df: pd.DataFrame):\n",
    "    return df.duplicated().sum()\n",
    "\n",
    "check_duplicate_rows(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there exists any duplicate rows without considering the loan_id and customer_id as both uniquely identify each row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate_rows_without_unique_columns(df: pd.DataFrame, columns: list):\n",
    "    # Drop the specified columns temporarily for duplicate checking\n",
    "    df_temp = df.drop(columns=columns)\n",
    "    # Find the duplicate rows based on the remaining columns\n",
    "    duplicates = df[df_temp.duplicated(keep=False)]\n",
    "    return duplicates\n",
    "\n",
    "check_duplicate_rows_without_unique_columns(df, ['loan_id', 'customer_id']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate rows in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExpectedDataTypes = {\n",
    "    \"customer_id\": \"object\",\n",
    "    \"emp_title\": \"object\",\n",
    "    \"emp_length\": \"object\",\n",
    "    \"home_ownership\": \"object\",\n",
    "    \"annual_inc\": \"float64\",\n",
    "    \"annual_inc_joint\": \"float64\",\n",
    "    \"verification_status\": \"object\",\n",
    "    \"zip_code\": \"object\",\n",
    "    \"addr_state\": \"object\",\n",
    "    \"avg_cur_bal\": \"float64\",\n",
    "    \"tot_cur_bal\": \"float64\",\n",
    "    \"loan_id\": \"int64\",\n",
    "    \"loan_status\": \"object\",\n",
    "    \"loan_amount\": \"float64\",\n",
    "    \"state\": \"object\",\n",
    "    \"funded_amount\": \"float64\",\n",
    "    \"term\": \"object\", \n",
    "    \"int_rate\": \"float64\",\n",
    "    \"grade\": \"int64\",\n",
    "    \"issue_date\": \"object\",\n",
    "    \"pymnt_plan\": \"bool\",\n",
    "    \"type\": \"object\",\n",
    "    \"purpose\": \"object\",\n",
    "    \"description\": \"object\"\n",
    "}\n",
    "\n",
    "def check_column_data_types(df: pd.DataFrame, expected_data_types=ExpectedDataTypes):\n",
    "    for column in df.columns:\n",
    "        actual_type = df[column].dtype\n",
    "        expected_type = expected_data_types.get(column)\n",
    "        if expected_type is None:\n",
    "            print(f'Failure: No expected data type for column {column}')\n",
    "        elif actual_type != expected_type:\n",
    "            print(f'Failure :Column {column} has data type {actual_type} but expected {expected_type}')\n",
    "        else:\n",
    "            print(f'Column {column} has expected data type {expected_type}')\n",
    "\n",
    "\n",
    "check_column_data_types(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for negative values in the dataset as columns that are numeric such as annual_inc, annual_inc_joint, avg_cur_bal, tot_cur_bal, loan_id, loan_amount, funded_amount, int_rate, and grade should not have negative values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_negative_numbers_in_numeric_columns(df: pd.DataFrame):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype in ['int64', 'float64']:\n",
    "            if df[column].lt(0).sum() > 0:\n",
    "                print(f'Column {column} has {df[column].lt(0).sum()} negative values.')\n",
    "\n",
    "check_negative_numbers_in_numeric_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no negative values in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the columns of type object hold any numeric values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numeric_in_object_columns(df: pd.DataFrame):\n",
    "    res = []\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            if df[column].str.isnumeric().sum() > 0:\n",
    "                print(f'Column {column} has {df[column].str.isnumeric().sum()} numeric values.')\n",
    "                print(column)\n",
    "                res += [column]\n",
    "\n",
    "    return res\n",
    "\n",
    "check_numeric_in_object_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Ask shown below, the emp_length column has numeric values\"\n",
    "test = df.dropna(subset=['emp_title'])\n",
    "test[test['emp_title'].str.isnumeric()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the value counts of the columns of type object to detect any irrelevant or incorrect data, different spelling with the same meaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_columns(df: pd.DataFrame):\n",
    "    categorical_value_counts = {}\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column in ['loan_id', 'customer_id']:\n",
    "            continue\n",
    "        if df[column].dtype == 'object' or df[column].dtype == 'bool':\n",
    "            categorical_value_counts[column] = df[column].value_counts()\n",
    "    \n",
    "    return categorical_value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_standardize = ['emp_title', 'home_ownership', 'verification_status','type']\n",
    "\n",
    "summarize_column_values(df, columns_to_standardize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to make the values of each column consistent by converting them to lowercase and capitalizing the first letter of each word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_values_proper_case(df: pd.DataFrame, columns: list):\n",
    "    for column in columns:\n",
    "        # if value is null or nan skip\n",
    "        df[column] = df[column].apply(lambda x: ' '.join([word.capitalize() for word in str(x).split()]) if pd.notnull(x) else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "columns_to_standardize = ['emp_title', 'home_ownership', 'verification_status', 'type']\n",
    "\n",
    "df = standardize_values_proper_case(df, columns_to_standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_column_values(df, columns_to_standardize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can all the columns have consistent values where each value is capitalized and the rest are lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.home_ownership.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.verification_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe if we can merge both source verified and verified into one class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the 3 classes among the grades of the customer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_countplot_multiple_columns(df, 'loan_status', 'verification_status', 'Relationship Between Verification Status and Grade', 'Grade', 'Count', (12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot_multiple_columns(df, 'verification_status', 'int_rate', '', 'verification_status', 'int_rate', (12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_countplot_multiple_columns(df, 'loan_status', 'verification_status', 'Relationship Between Verification Status and loan_status', 'loan_status', 'Count', (12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loan_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.grade.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_column_values(df, ['type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the column 'type' Joint App and Joint are the same so I will replace Joint App with Joint and Direct_pay with Direct Pay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_column_values_to_mapped_values(df: pd.DataFrame, column: str, mapping_dict: dict):\n",
    "    df[column] = df[column].map(mapping_dict)\n",
    "    return df\n",
    "\n",
    "type_column_map = {\n",
    "    \"Individual\": \"Individual\",\n",
    "    \"Joint\": \"Joint\",\n",
    "    \"Joint App\": \"Joint\",\n",
    "    \"Direct_pay\": \"Direct Pay\"\n",
    "}\n",
    "\n",
    "df = change_column_values_to_mapped_values(df, 'type', type_column_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_column_values(df, ['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_column_values(df, ['issue_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the issue date, we need to check contain valid number of days in each month and year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_date_values(df: pd.DataFrame, column: str):\n",
    "    invalid_dates = []\n",
    "    for idx, date in enumerate(df[column]):\n",
    "        try:\n",
    "            pd.to_datetime(date)\n",
    "        except:\n",
    "            invalid_dates.append((idx, date))\n",
    "        \n",
    "    return invalid_dates\n",
    "\n",
    "validate_date_values(df, 'issue_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all values of the issue date are valid, what we did we looped over the values of the issue date and tried to cast to datetime if it fails we will append the location of the value to the invalid_dates list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings and conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing Missing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_count_and_percentage(df: pd.DataFrame, percentage=False):\n",
    "    \"\"\"\n",
    "    Returns the count or percentage of missing values for each column with null entries.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The DataFrame to analyze.\n",
    "    percentage (bool): If True, returns the percentage of null values per column. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: A Series showing either the count or percentage of missing values for columns with null values.\n",
    "    \"\"\"\n",
    "    null_cnt = df.isnull().sum()\n",
    "\n",
    "    null_cnt = null_cnt[null_cnt > 0]\n",
    "    if percentage:\n",
    "        null_cnt = null_cnt / len(df) * 100\n",
    "    return null_cnt\n",
    "\n",
    "get_null_count_and_percentage(df), get_null_count_and_percentage(df, percentage=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_non_standard_missing_values(df: pd.DataFrame, columns: list):\n",
    "    \"\"\"\n",
    "    Checks for non-standard missing values in the specified columns of a dataframe.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The dataframe to check for non-standard missing values.\n",
    "    columns (list): List of column names to check for non-standard missing values.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where each key is a column name, and each value is a dictionary containing:\n",
    "          - 'values': A list of unique non-standard missing values found.\n",
    "          - 'count': The count of occurrences for non-standard missing values in that column.\n",
    "    \"\"\"\n",
    "    # Define the non-standard missing values to search for\n",
    "    non_standard_missing_values = [\"na\", \"n/a\", \"missing\", \"none\", \"nan\", \"null\", \"nil\"]\n",
    "    \n",
    "    # Store found values and counts in a dictionary\n",
    "    missing_values_dict = {}\n",
    "    \n",
    "    for column in columns:\n",
    "        # Initialize sets for non-standard missing values and their counts\n",
    "        found_values = set()\n",
    "        count = 0\n",
    "        \n",
    "        # Check for each non-standard missing value in the column\n",
    "        for value in non_standard_missing_values:\n",
    "            # Filter for exact matches using case-insensitive comparison\n",
    "            matches = df[df[column].astype(str).str.lower() == value]\n",
    "            \n",
    "            # Add unique matches to found_values and update the count\n",
    "            found_values.update(matches[column].unique())\n",
    "            count += len(matches)\n",
    "        \n",
    "        # Store the unique values and their count for this column\n",
    "        if found_values:\n",
    "            missing_values_dict[column] = {'values': list(found_values), 'count': count}\n",
    "    \n",
    "    return missing_values_dict\n",
    "\n",
    "check_for_non_standard_missing_values(df, df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_null_count_and_percentage(df).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5 columns with missing data which are annual_inc_joint, emp_title, emp_length, int_rate, and description. We will handle the missing data in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot missing data heatmap\n",
    "def plot_missing_data_heatmap(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of missing data in the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Dataframe to analyze.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
    "    plt.title('Missing Data Heatmap')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Rows')\n",
    "    plt.show()\n",
    "\n",
    "def draw_correlation_map(df: pd.DataFrame, values_interest: list):\n",
    "    correlation_matrix = df[values_interest].corr()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Create the heatmap\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.xlabel('Variables')\n",
    "    plt.ylabel('')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show() \n",
    "\n",
    "# Function to analyze correlation of missing data with other attributes\n",
    "def analyze_missing_correlation(df: pd.DataFrame, target_column: str):\n",
    "    \"\"\"\n",
    "    Analyzes the correlation between the missing status of the target column and other columns in the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Dataframe to analyze.\n",
    "    target_column (str): Column name to analyze the missing data correlation.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Correlation dataframe with missing indicator for the target column.\n",
    "    \"\"\"\n",
    "    # Create a missing indicator for the target column\n",
    "    df_missing_indicator = df.copy()\n",
    "    df_missing_indicator[target_column + '_missing'] = df[target_column].isnull().astype(int)\n",
    "    \n",
    "    # Calculate correlations with the missing indicator\n",
    "    missing_correlation = df_missing_indicator.corr()[target_column + '_missing'].sort_values(ascending=False)\n",
    "    \n",
    "    # Plot the correlation heatmap with missing indicator\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(missing_correlation.to_frame(), annot=True, cmap='coolwarm', cbar=True)\n",
    "    plt.title(f'Correlation of {target_column} Missing Indicator with Other Features')\n",
    "    plt.show()\n",
    "\n",
    "    return missing_correlation\n",
    "\n",
    "def plot_missing_vs_categoricals(df: pd.DataFrame, target_column: str, cat_columns: list):\n",
    "    \"\"\"\n",
    "    Plots the relationship between a missing attribute and multiple categorical attributes, each in its own row.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame to analyze.\n",
    "    target_column (str): Column with missing values to analyze.\n",
    "    cat_columns (list): List of categorical columns to compare the missing values with.\n",
    "    \"\"\"\n",
    "    # Create a missing indicator for the target column\n",
    "    df_missing = df.copy()\n",
    "    df_missing[target_column + '_missing'] = df[target_column].isnull().astype(int)\n",
    "    \n",
    "    # Set the number of plots\n",
    "    num_cols = len(cat_columns)\n",
    "\n",
    "    plt.figure(figsize=(10, 5 * num_cols))  # Adjust height based on the number of columns\n",
    "\n",
    "    # Plot each categorical column in a separate row\n",
    "    for i, cat_column in enumerate(cat_columns):\n",
    "        plt.subplot(num_cols, 1, i + 1)  # Create a subplot for each categorical column in one column\n",
    "        sns.barplot(x=cat_column, y=target_column + '_missing', data=df_missing, ci=None)\n",
    "        plt.title(f'Relationship between {target_column} Missing Indicator and {cat_column}')\n",
    "        plt.ylabel(f'Proportion of Missing in {target_column}')\n",
    "        plt.xlabel(cat_column)\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_categorical_vs_categorical(df: pd.DataFrame, cat_column1: str, cat_column2: str):\n",
    "    \"\"\"\n",
    "    Plots the relationship between two categorical attributes.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame to analyze.\n",
    "    cat_column1 (str): First categorical column to analyze.\n",
    "    cat_column2 (str): Second categorical column to compare with.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Count plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(x=cat_column1, hue=cat_column2, data=df, palette=\"viridis\")\n",
    "    plt.title(f'Count of {cat_column1} by {cat_column2}')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel(cat_column1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=cat_column2)\n",
    "\n",
    "    # Stacked Bar Plot\n",
    "    counts = df.groupby([cat_column1, cat_column2]).size().unstack(fill_value=0)\n",
    "    counts.plot(kind='bar', stacked=True, ax=plt.subplot(1, 2, 2), colormap='viridis')\n",
    "    plt.title(f'Stacked Bar Plot of {cat_column1} by {cat_column2}')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel(cat_column1)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_correlations(df, target_column, reference_columns):\n",
    "    \"\"\"\n",
    "    Plots the correlation between a target column and multiple reference columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The data frame to analyze.\n",
    "        target_column (str): The column to plot correlations against.\n",
    "        reference_columns (list): A list of columns to compare with the target column.\n",
    "        \n",
    "    Returns:\n",
    "        None: Shows the plots.\n",
    "    \"\"\"\n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(15, 5 * len(reference_columns)))\n",
    "    \n",
    "    # Iterate over each reference column to plot correlation\n",
    "    for i, ref_col in enumerate(reference_columns, start=1):\n",
    "        plt.subplot(len(reference_columns), 1, i)\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(df[ref_col]):\n",
    "            # Scatter plot for numeric reference columns\n",
    "            sns.scatterplot(data=df, x=ref_col, y=target_column)\n",
    "            plt.title(f'Scatter Plot: {target_column} vs {ref_col}')\n",
    "            plt.xlabel(ref_col)\n",
    "            plt.ylabel(target_column)\n",
    "        \n",
    "        elif pd.api.types.is_categorical_dtype(df[ref_col]) or df[ref_col].dtype == 'object':\n",
    "            # Box plot for categorical reference columns\n",
    "            sns.boxplot(data=df, x=ref_col, y=target_column)\n",
    "            plt.title(f'Box Plot: {target_column} vs {ref_col}')\n",
    "            plt.xlabel(ref_col)\n",
    "            plt.ylabel(target_column)\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        else:\n",
    "            print(f\"Column {ref_col} has unsupported data type for plotting. Skipping...\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cols_values_cnt(dataset, col_name, percentage=False, title=None):\n",
    "    (dataset[col_name].value_counts(dropna=False) * ((100/len(dataset)) if percentage else 1) ).plot(kind='bar', xlabel=col_name, ylabel='count', title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_data_heatmap(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start by investigating the missing data in the annual_inc_joint column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the count of null values of annual_inc_joint in each loan type\n",
    "df.groupby('type')['annual_inc_joint'].apply(lambda x: x.isnull().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in the annual_inc_joint with the loan type being joint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_count = df[df['type'] == 'Individual']['type'].count()\n",
    "direct_pay_count =df[df['type'] == 'Direct Pay']['type'].count()\n",
    "\n",
    "individual_count+direct_pay_count == df[df['annual_inc_joint'].isnull()]['type'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['annual_inc_joint'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about the loan types in the dataset. We have 3 types of loans which are:\n",
    "\n",
    "#### 1. Individual Loan\n",
    "\n",
    "**Definition:**  \n",
    "An individual loan is taken out by a single borrower, who is solely responsible for repaying the loan.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Joint Loan\n",
    "\n",
    "**Definition:**  \n",
    "A joint loan is applied for and signed by two or more borrowers, such as spouses or business partners. All individuals on the loan are responsible for repaying it.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Direct Pay Loan\n",
    "\n",
    "**Definition:**  \n",
    "In a direct pay loan, the lender pays the funds directly to the institution or organization on behalf of the borrower, rather than disbursing the funds to the borrower.\n",
    "\n",
    "---\n",
    "\n",
    "#### We have observed that the annual_inc_joint column is missing for all individual and direct_pay loans. This is because these types of loans do not have a joint applicant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Annual Inc Joint Column**: This data is missing not at random (MNAR) as the missing values are dependent on the loan type.\n",
    "\n",
    "- We can fill the missing values in the annual_inc_joint column with 0 for individual and direct_pay loans.\n",
    "- As 0 will act as special character indicating the loan_type is individual or direct_pay loan which does not have a joint applicant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's investigate the missing data in the empl_title column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emp_title'].isna().sum(), df['emp_title'].isna().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of missing values in the emp_title column is 2376.\n",
    "- The percentage of missing values in the emp_title column is 8.8%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['emp_title'].isna()].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['emp_title'].isna()].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emp_title'].isna().groupby(df['emp_length']).mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['emp_title'].isna()].groupby('emp_length')['emp_length'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_emp_title_by_length = df.groupby('emp_length')['emp_title'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "mode_emp_title_by_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['emp_length'] == '< 1 year'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_vs_categoricals(df, 'emp_title', ['emp_length', 'home_ownership', 'verification_status', 'addr_state', 'loan_status', 'state', 'type', 'grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's investigate the missing data in the emp_length column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['emp_length'].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['emp_length'].isna()].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's investigate the missing data in the int_rate column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['int_rate'].isna().sum(), df['int_rate'].isna().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of missing values in the int_rate column is 1211.\n",
    "- The percentage of missing values in the int_rate column is 4.48%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['int_rate'].isna()].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['int_rate'].isna()].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.int_rate.isna().groupby(df['grade']).mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.int_rate.isna().groupby(df['loan_status']).mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.int_rate.isna().groupby(df['term']).mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_missing_correlation(df, 'int_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlations(df, 'int_rate', ['grade', 'loan_amount', 'annual_inc', 'loan_status', 'verification_status', 'term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_vs_multiple_categoricals(df, 'int_rate', ['loan_status', 'grade', 'type', 'verification_status', 'term', 'purpose'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Int rate Column**: This data is missing completely at random as there are no relationship between the data missing and any other values, observed or missing, within the dataset. I believe it is missing because it was not recorded. Loans must be associated with an interest rate, so it is important to fill in the missing values.\n",
    "\n",
    "- We can fill the missing values in the int_rate column with the mean value of the column grouped by the loan grade. As there is a strong relationship between the loan grade and the interest rate, we can use the mean interest rate for each loan grade to fill in the missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's investigate the missing data in the description column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'].isna().sum(), df['description'].isna().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of missing values in the int_rate column is 218.\n",
    "- The percentage of missing values in the int_rate column is 0.8%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['description'].isna()].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['description'].isna()].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.description.isna().groupby(df['purpose']).mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.description.isna().groupby(df['home_ownership']).mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_vs_categoricals(df, 'description', ['purpose'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Description Column**: This data is missing completely at random as there are no relationship between the data missing and any other values, observed or missing, within the dataset. Also the description may was left as optional field so many customers may not have filled it. The percentage of the missed values are very low (0.8%)\n",
    "\n",
    "- We can fill the missing values in the description by grouping the data by the purpose of the loan and filling the missing values with the most common description for that purpose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings and conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings and conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Data transformation and feature eng.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Adding Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.22 - Findings and conlcusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.31 - Findings and conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Lookup Table(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Bonus ( Data Integration )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Exporting the dataframe to a csv file or parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
